{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-01-20 11:53:44,662] [    INFO] - Installing faster_rcnn_coco2017 module\n",
      "[2020-01-20 11:53:44,676] [    INFO] - Module faster_rcnn_coco2017 already installed in /home/paperspace/.paddlehub/modules/faster_rcnn_coco2017\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "from math import sin, cos\n",
    "from PIL import ImageDraw, Image\n",
    "import cv2\n",
    "sys.path.append(os.path.abspath(\"./Residual-Attention-Network/\"))\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import time\n",
    "import sklearn.preprocessing\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torchvision\n",
    "from torchvision.models import resnet18\n",
    "module = hub.Module(name=\"faster_rcnn_coco2017\")\n",
    "%autosave 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2b6b983d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def rotate(x, angle):\n",
    "    x = x + angle\n",
    "    x = x - (x + np.pi) // (2 * np.pi) * 2 * np.pi\n",
    "    return x\n",
    "def pad_img(img, size = (112, 112, 3)):\n",
    "    padded_img = np.zeros(size)\n",
    "    pad_center_y = size[0]/2\n",
    "    pad_center_x = size[1]/2\n",
    "    if img.shape[0]>img.shape[1]:\n",
    "        newwidth = int(img.shape[1]/img.shape[0]*size[0]//2*2)\n",
    "        img = cv2.resize(img, (newwidth, size[0]))\n",
    "        padded_img[:, int(pad_center_x-newwidth//2):int(pad_center_x+newwidth//2)] = img\n",
    "    else:\n",
    "        newheight = int(img.shape[0]/img.shape[1]*size[1]//2*2)\n",
    "        img = cv2.resize(img, (size[1], newheight))\n",
    "        padded_img[int(pad_center_y-newheight//2):int(pad_center_y+newheight//2), :] = img\n",
    "    return padded_img\n",
    "def get_numpy_mesh(shape_y, shape_x, box):\n",
    "    mesh = np.zeros([shape_y, shape_x, 2])\n",
    "    \n",
    "    xmax, ymax, xmin, ymin = box\n",
    "    mg_y, mg_x = np.meshgrid(np.linspace(ymin, ymax, shape_y), np.linspace(xmin, xmax, shape_x), indexing = 'ij')\n",
    "    mesh[:, :, 0] = mg_y\n",
    "    mesh[:, :, 1] = mg_x\n",
    "    return mesh\n",
    "class stage2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mode = 'train'):\n",
    "        super(stage2Dataset, self)\n",
    "        self.df = pd.read_csv('./Joseph_Dataset/cropped_rcnn_annotations.csv')\n",
    "#         self.means = {}\n",
    "#         self.stds = {}\n",
    "#         keys = ['x', 'y', 'z']\n",
    "#         for key in keys:\n",
    "#             self.means[key] = np.mean(df[key])\n",
    "#             self.stds[key] = np.std(df[key])\n",
    "        if mode == 'train':\n",
    "            self.df = self.df[:int(len(self.df)*0.7)]  \n",
    "        else:\n",
    "            self.df = self.df[int(len(self.df)*0.7):]   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        dic = dict(self.df.iloc[idx])\n",
    "        keys = ['pred_filename','xmin', 'xmax', 'xcenter', 'ymin', 'ymax', 'ycenter', 'yaw', 'pitch', 'roll', 'x', 'y', 'z']\n",
    "        pred_filename, xmin, xmax, xcenter, ymin, ymax, ycenter, yaw, pitch, roll, x, y, z = [dic[key] for key in keys]\n",
    "        img = plt.imread(pred_filename).astype('float32')/255\n",
    "        height, width, channels = img.shape\n",
    "        mesh = get_numpy_mesh(height, width,[xmax/3384, ymax/2710, xmin/3384, ymin/2710])\n",
    "        img = np.concatenate([img, mesh], axis = 2)\n",
    "        img = pad_img(img, size = (112, 112, 5))\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        xcenter = (xcenter-xmin)/(xmax-xmin)\n",
    "        ycenter = (ycenter-ymin)/(ymax-ymin)\n",
    "        xcenter = xcenter*2-1\n",
    "        ycenter = ycenter*2-1\n",
    "#         keys = ['x', 'y', 'z']\n",
    "#         means = np.array([self.means[key] for key in keys])\n",
    "#         stds = np.array([self.stds[key] for key in keys])\n",
    "        return img,  (np.array([np.cos(yaw)>0, np.sin(yaw)*np.round(np.cos(yaw)), pitch, rotate(roll, np.pi), xcenter, ycenter])).astype('float32')\n",
    "\n",
    "stage2set = stage2Dataset()\n",
    "trainloader = torch.utils.data.DataLoader(stage2set, batch_size=128, shuffle = True)\n",
    "plt.imshow(stage2set[0][0].transpose((1, 2, 0))[:, :, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from model.basic_layers import ResidualBlock\n",
    "from model.attention_module import AttentionModule_stage1, AttentionModule_stage2, AttentionModule_stage3, AttentionModule_stage0\n",
    "from model.attention_module import AttentionModule_stage1_cifar, AttentionModule_stage2_cifar, AttentionModule_stage3_cifar\n",
    "class ResidualAttentionModel_56(nn.Module):\n",
    "    # for input size 224\n",
    "    def __init__(self):\n",
    "        super(ResidualAttentionModel_56, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(5, 64, kernel_size=7, stride=2, padding=3, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.residual_block1 = ResidualBlock(64, 256)\n",
    "        self.attention_module1 = AttentionModule_stage1(256, 256)\n",
    "        self.residual_block2 = ResidualBlock(256, 512, 2)\n",
    "        self.attention_module2 = AttentionModule_stage2(512, 512)\n",
    "        self.residual_block3 = ResidualBlock(512, 1024, 2)\n",
    "        self.attention_module3 = AttentionModule_stage3(1024, 1024)\n",
    "        self.residual_block4 = ResidualBlock(1024, 2048, 2)\n",
    "        self.residual_block5 = ResidualBlock(2048, 2048)\n",
    "        self.residual_block6 = ResidualBlock(2048, 2048)\n",
    "        self.mpool2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(2048,7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.mpool1(out)\n",
    "        # print(out.data)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.attention_module1(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.attention_module2(out)\n",
    "        out = self.residual_block3(out)\n",
    "        # print(out.data)\n",
    "        out = self.attention_module3(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.residual_block6(out)\n",
    "        out = self.mpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mesh(batch_size, shape_y, shape_x, box_specs):\n",
    "    mesh = torch.zeros([batch_size, 2, shape_y, shape_x])\n",
    "    for i in range(batch_size):\n",
    "        xmax, ymax, xmin, ymin = box_specs[i].detach().numpy()\n",
    "        mg_y, mg_x = np.meshgrid(np.linspace(ymin, ymax, shape_y), np.linspace(xmin, xmax, shape_x), indexing = 'ij')\n",
    "        mesh[i, 0, :, :] = torch.Tensor(mg_y)\n",
    "        mesh[i, 1, :, :] = torch.Tensor(mg_x)\n",
    "    return mesh\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=3):\n",
    "        self.inplanes = 64 #64\n",
    "        super(ResNet, self).__init__()\n",
    "#        self.simplemodel = reallysimplemodel()\n",
    "        self.conv1 = nn.Conv2d(5, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn0 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.selu = nn.SELU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0]) #64\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) #128\n",
    "        self.attention_module3 = AttentionModule_stage3(128, 128)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2) #256\n",
    "        self.attention_module4 = AttentionModule_stage3(256, 256, (7, 7))\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2) #512\n",
    "        self.avgpool = nn.AvgPool2d(4, stride=1)\n",
    "        self.fc = nn.Linear(8192 * block.expansion, 200)\n",
    "        self.fc1 = nn.Linear(200, 6)\n",
    "        #self.bn1 = nn.BatchNorm1d(300)\n",
    "#         self.fc2 = nn.Linear(300, 500)\n",
    "#         self.bn2 = nn.BatchNorm1d(500)\n",
    "#         self.fc3 = nn.Linear(500, 300)\n",
    "#         self.bn3 = nn.BatchNorm1d(300)\n",
    "#         self.fc4 = nn.Linear(300, 200)\n",
    "#         self.bn4 = nn.BatchNorm1d(200)\n",
    "#         self.fc5 = nn.Linear(200, 200)\n",
    "#         self.bn5 = nn.BatchNorm1d(200)\n",
    "        #self.fc6 = nn.Linear(300, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        ## to compensate for the meshgrid for next round\n",
    "        #self.inplanes += 2\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x, box_specs):\n",
    "        #batch_size = len(box_specs)\n",
    "        #mesh = get_mesh(batch_size, x.shape[2], x.shape[3], box_specs)\n",
    "        #x = torch.cat([x, mesh], dim = 1)\n",
    "        \n",
    "        \n",
    "        x = self.conv1(x)    # 224x224\n",
    "        x = self.bn0(x)     \n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)  # 112x112\n",
    "        \n",
    "        #mesh = get_mesh(batch_size, x.shape[2], x.shape[3], box_specs)\n",
    "        #x = torch.cat([x, mesh], 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.layer1(x)   # 56x56\n",
    "        #mesh = get_mesh(batch_size, x.shape[2], x.shape[3], box_specs)\n",
    "        #x = torch.cat([x, mesh], 1)\n",
    "        \n",
    "        \n",
    "        x = self.layer2(x)   # 28x28\n",
    "        x = self.attention_module3(x)\n",
    "\n",
    "        #mesh = get_mesh(batch_size, x.shape[2], x.shape[3], box_specs)\n",
    "        #x = torch.cat([x, mesh], 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = self.layer3(x)   # 14x14\n",
    "        #x = self.attention_module4(x)\n",
    "        #mesh = get_mesh(batch_size, x.shape[2], x.shape[3], box_specs)\n",
    "        #x = torch.cat([x, mesh], 1)\n",
    "        \n",
    "        x = self.layer4(x)   # 7x7\n",
    "        #mesh = get_mesh(batch_size, x.shape[2], x.shape[3], box_specs)\n",
    "        #x = torch.cat([x, mesh], 1)\n",
    "        #x = self.avgpool(x)  # 1x1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        X = self.relu(self.fc(x))\n",
    "        X = self.fc1(X)\n",
    "        return X\n",
    "\n",
    "class reallysimplemodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(reallysimplemodel, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 300)\n",
    "        self.fc2 = nn.Linear(300, 500)\n",
    "        self.fc3 = nn.Linear(500, 300)\n",
    "        self.fc4 = nn.Linear(300, 200)\n",
    "        self.fc5 = nn.Linear(200, 200)\n",
    "        self.fc6 = nn.Linear(200, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "    def forward(self, X):\n",
    "#         X1 = torch.Tensor(X)\n",
    "#         X1[:, 0] = X[:, 0]/2+X[:, 2]/2\n",
    "#         X1[:, 1] = X[:, 1]/2+X[:, 3]/2\n",
    "#         X1[:, 2] = X[:, 0]-X[:, 2]\n",
    "#         X1[:, 3] = X[:, 1] - X[:, 3]\n",
    "        X1 = self.relu(self.fc1(X))\n",
    "        X = self.fc2(X1)\n",
    "        X = self.relu(self.fc3(X))\n",
    "        X += X1\n",
    "        X = self.fc4(X)\n",
    "        X = self.selu(self.fc5(X))\n",
    "        out = self.fc6(X)\n",
    "        return out\n",
    "            \n",
    "# inp = k.layers.Input(shape=(4,))\n",
    "# h1 = k.layers.Dense(300, activation='relu')(inp)\n",
    "# h = k.layers.Dense(500, activation='selu')(h1)\n",
    "# h = k.layers.Dense(300, activation='relu')(h)\n",
    "# h += h1 #residue\n",
    "# h = k.layers.Dense(200, activation='relu')(h)\n",
    "# h = k.layers.Dense(200, activation='selu')(h)\n",
    "# out = k.layers.Dense(3, activation='linear')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = ResidualAttentionModel_56() #this is the attention version, which is much slower\n",
    "net = ResNet(BasicBlock, [1, 1, 1, 1]) #[2,2,2,2]\n",
    "net.train()\n",
    "for parameter in net.parameters():\n",
    "    if len(parameter.shape)>1:\n",
    "        torch.nn.init.xavier_uniform_(parameter)\n",
    "net = torch.load('./Joseph_Dataset/weights/last_attention.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestloss = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True) \n",
    "reg_weights = [1, 1, 1, 3, 3]\n",
    "reg_weights = torch.Tensor(reg_weights)\n",
    "stage2set = stage2Dataset()\n",
    "trainloader = torch.utils.data.DataLoader(stage2set, batch_size=32, shuffle = True) \n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr = 0.0001) \n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, 0.000001, 0.0001, step_size_up = 200, mode = 'triangular2', cycle_momentum=False, last_epoch = -1)\n",
    "L1 = torch.nn.L1Loss()\n",
    "BCE = torch.nn.BCEWithLogitsLoss()\n",
    "idx = 0\n",
    "for epoch in range(50):\n",
    "    for (images, box_specs, target) in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images, box_specs)\n",
    "        clsloss = BCE(outputs[:, 0], target[:, 0])\n",
    "        regloss = L1(outputs[:, 1:], target[:, 1:])\n",
    "        loss = clsloss + regloss\n",
    "        print('clsloss',clsloss.detach().numpy(), 'regloss',regloss.detach().numpy(), end = '\\t')\n",
    "        #print accuracy for binary classification\n",
    "        print('accuracy', sum((outputs[:,0].detach().numpy()>0)==target[:,0].detach().numpy()).astype(int)/32, end = '\\t')\n",
    "        if loss.detach().numpy()<bestloss:\n",
    "            bestloss = loss.detach().numpy()\n",
    "            torch.save(net, './Joseph_Dataset/weights/best_attention.pth')\n",
    "            print(str(loss.detach().numpy()), 'saved!', end = '\\r')\n",
    "        else:\n",
    "            print('totalloss', str(loss.detach().numpy())+'               ', end = '\\r')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if idx%30==0:\n",
    "            print('\\n', outputs[0].detach().numpy(), target[0].detach().numpy())\n",
    "        idx+=1\n",
    "    torch.save(net, './Joseph_Dataset/weights/last_attention.pth')\n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = torch.load('./Joseph_Dataset/weights/net.pth')\n",
    "#visualize\n",
    "k = np.array([[2304.5479, 0,  1686.2379],\n",
    "           [0, 2305.8757, 1354.9849],\n",
    "           [0, 0, 1]], dtype = np.float32)\n",
    "testfile = './train_images/ID_a381bf4d0.jpg'\n",
    "image = plt.imread(testfile)\n",
    "filenames, xmin, xmax, xcenter, ymin, ymax, ycenter, x, y, z = df.loc[df['original_name']==testfile][['cropped_filename','xmin','xmax', 'xcenter', 'ymin','ymax', 'ycenter', 'x', 'y', 'z']].values.T\n",
    "predictors = np.zeros((len(xmin), 4))\n",
    "predictors[:, 0] = xmax.astype(int)/3384\n",
    "predictors[:, 1] = ymax.astype(int)/2710\n",
    "predictors[:, 2] = xmin.astype(int)/3384\n",
    "predictors[:, 3] = ymin.astype(int)/2710\n",
    "images = np.zeros((len(filenames), 3, 112, 112))\n",
    "for i, filename in enumerate(filenames):\n",
    "    img = plt.imread(filename)\n",
    "    img = cv2.resize(img, (112, 112))\n",
    "    img = img.astype('float32').transpose((2, 0, 1))/255\n",
    "    images[i] = img\n",
    "images = torch.Tensor(images)\n",
    "preds = net(images, torch.Tensor(predictors)).detach().numpy().astype('float32')\n",
    "preds = preds*100\n",
    "camera_xyz = np.dot(k, preds.T).T\n",
    "camera_xyz[:, 0]/=camera_xyz[:, 2]\n",
    "camera_xyz[:, 1]/=camera_xyz[:, 2]\n",
    "img_xyz = camera_xyz[:, :2]\n",
    "print(img_xyz.shape)\n",
    "for [p_x, p_y] in img_xyz:\n",
    "    cv2.circle(image, (int(p_x), int(p_y)), 5, (255, 0, 0), -1)\n",
    "plt.figure(figsize = (30, 20))\n",
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
