{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras import backend\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from math import sin, cos\n",
    "from PIL import ImageDraw, Image\n",
    "import cv2\n",
    "sys.path.append(os.path.abspath(\"./Residual-Attention-Network/\"))\n",
    "from model.residual_attention_network import ResidualAttentionModel_92 as ResidualAttentionModel\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "# k is camera instrinsic matrix\n",
    "k = np.array([[2304.5479, 0,  1686.2379],\n",
    "           [0, 2305.8757, 1354.9849],\n",
    "           [0, 0, 1]], dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert euler angle to rotation matrix\n",
    "def euler_to_Rot(yaw, pitch, roll):\n",
    "    Y = np.array([[cos(yaw), 0, sin(yaw)],\n",
    "                  [0, 1, 0],\n",
    "                  [-sin(yaw), 0, cos(yaw)]])\n",
    "    P = np.array([[1, 0, 0],\n",
    "                  [0, cos(pitch), -sin(pitch)],\n",
    "                  [0, sin(pitch), cos(pitch)]])\n",
    "    R = np.array([[cos(roll), -sin(roll), 0],\n",
    "                  [sin(roll), cos(roll), 0],\n",
    "                  [0, 0, 1]])\n",
    "    return np.dot(Y, np.dot(P, R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(image, points):\n",
    "    color = (255, 0, 0)\n",
    "    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[1][:2]), tuple(points[4][:2]), color, 16)\n",
    "\n",
    "    cv2.line(image, tuple(points[1][:2]), tuple(points[5][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[2][:2]), tuple(points[6][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[3][:2]), tuple(points[4][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[3][:2]), tuple(points[7][:2]), color, 16)\n",
    "\n",
    "    cv2.line(image, tuple(points[4][:2]), tuple(points[8][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[5][:2]), tuple(points[8][:2]), color, 16)\n",
    "\n",
    "    cv2.line(image, tuple(points[5][:2]), tuple(points[6][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[6][:2]), tuple(points[7][:2]), color, 16)\n",
    "    cv2.line(image, tuple(points[7][:2]), tuple(points[8][:2]), color, 16)\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_points(image, points):\n",
    "    image = np.array(image)\n",
    "    for (p_x, p_y) in points:\n",
    "        # print(\"p_x, p_y\", p_x, p_y)\n",
    "        cv2.circle(image, (p_x, p_y), 5, (255, 0, 0), -1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image coordinate to world coordinate\n",
    "def img_cor_2_world_cor():\n",
    "    x_img, y_img, z_img = img_cor_points[0]\n",
    "    xc, yc, zc = x_img*z_img, y_img*z_img, z_img\n",
    "    p_cam = np.array([xc, yc, zc])\n",
    "    xw, yw, zw = np.dot(np.linalg.inv(k), p_cam)\n",
    "    print(xw, yw, zw)\n",
    "    print(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now begin the initialization of dictionary for image training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_MAX = 3383\n",
    "Y_MAX = 2709\n",
    "class stage2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, mode = 'train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.train = pd.read_csv(csv_file)\n",
    "        if mode == 'train':\n",
    "            self.train = self.train[:int(self.train.shape[0]*0.7)]\n",
    "        else:\n",
    "            self.train = self.train[int(self.train.shape[0]*0.7):]\n",
    "    def __len__(self):\n",
    "        return self.train.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_l = 1.02 # augment this\n",
    "        y_l = 0.80 # augment this\n",
    "        z_l = 2.31 # augment this\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = train.loc[idx]['ImageId']\n",
    "        pred_string = train.loc[idx]['PredictionString']\n",
    "        img = plt.imread('./train_images/' + img_name + '.jpg').astype(float)/255\n",
    "        mg_y, mg_x = np.meshgrid(np.linspace(0, 1, img.shape[0]), np.linspace(0, 1, img.shape[1]), indexing = 'ij')\n",
    "        #process prediction string\n",
    "        items = pred_string.split(' ')\n",
    "        model_types, yaws, pitches, rolls, xs, ys, zs = [items[i::7] for i in range(7)]\n",
    "        sample = []\n",
    "        #iterate over each vehcicle\n",
    "        for yaw, pitch, roll, x, y, z in zip(yaws, pitches, rolls, xs, ys, zs):\n",
    "            yaw, pitch, roll, x, y, z = [float(x) for x in [yaw, pitch, roll, x, y, z]]\n",
    "            img_x, img_y, img_z = np.dot(k, [x, y, z])\n",
    "            target = [yaw, pitch, roll, img_x/10000, img_y/10000, img_z] #result to regress to\n",
    "            # I think the pitch and yaw should be exchanged\n",
    "            yaw, pitch, roll = -pitch, -yaw, -roll\n",
    "            Rt = np.eye(4)\n",
    "            t = np.array([x, y, z])\n",
    "            Rt[:3, 3] = t\n",
    "            Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n",
    "            Rt = Rt[:3, :]\n",
    "            P = np.array([[0, 0, 0, 1],\n",
    "                          [x_l, y_l, -z_l, 1],\n",
    "                          [x_l, y_l, z_l, 1],\n",
    "                          [-x_l, y_l, z_l, 1],\n",
    "                          [-x_l, y_l, -z_l, 1],\n",
    "                          [x_l, -y_l, -z_l, 1],\n",
    "                          [x_l, -y_l, z_l, 1],\n",
    "                          [-x_l, -y_l, z_l, 1],\n",
    "                          [-x_l, -y_l, -z_l, 1]]).T\n",
    "\n",
    "            # call this function before change the astype\n",
    "            img_cor_points = np.dot(k, np.dot(Rt, P)) # calculate 8 corners and center in terms of camera coordinate\n",
    "            img_cor_points = img_cor_points.T\n",
    "            img_cor_points[:, 0] /= img_cor_points[:, 2] # project onto 2d camera plane\n",
    "            img_cor_points[:, 1] /= img_cor_points[:, 2]\n",
    "            x_max = int(np.amax(img_cor_points[:,0])) # calculate 2d box\n",
    "            if x_max>X_MAX:\n",
    "                x_max = X_MAX\n",
    "            x_min = int(np.amin(img_cor_points[:,0]))\n",
    "            if x_min<0:\n",
    "                x_min = 0\n",
    "            y_max = int(np.amax(img_cor_points[:,1]))\n",
    "            if y_max>Y_MAX:\n",
    "                y_max = Y_MAX\n",
    "            y_min = int(np.amin(img_cor_points[:,1])) \n",
    "            if y_min<0:\n",
    "                y_min = 0\n",
    "            # Crop image\n",
    "            img_crop = cv2.resize(img[y_min:y_max, x_min:x_max], (224, 224))\n",
    "            mg_x_crop = cv2.resize(mg_x[y_min:y_max, x_min:x_max], (224, 224))[..., np.newaxis]\n",
    "            mg_y_crop = cv2.resize(mg_y[y_min:y_max, x_min:x_max], (224, 224))[..., np.newaxis]\n",
    "            input_tensor = np.concatenate((img_crop, mg_y_crop, mg_x_crop), axis = 2).transpose((2, 0, 1))\n",
    "            sample.append((input_tensor, target))\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2set = stage2Dataset('./train.csv')\n",
    "trainloader = torch.utils.data.DataLoader(stage2set, batch_size=1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (float, int, int, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n * (object data, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0eeabdccaf13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResidualAttentionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pku-autonomous-driving/Residual-Attention-Network/model/residual_attention_network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_block1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResidualBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_module1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionModule_stage1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_block2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResidualBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/pku-autonomous-driving/Residual-Attention-Network/model/basic_layers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_channels, output_channels, stride)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_channels\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\u001b[0m\n\u001b[1;32m    325\u001b[0m         super(Conv2d, self).__init__(\n\u001b[1;32m    326\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             False, _pair(0), groups, bias, padding_mode)\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             self.weight = Parameter(torch.Tensor(\n\u001b[0;32m---> 40\u001b[0;31m                 out_channels, in_channels // groups, *kernel_size))\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (float, int, int, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n * (object data, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "net = ResidualAttentionModel()\n",
    "for batch_idx, sample in enumerate(trainloader):\n",
    "    X = torch.zeros([len(sample), 5, 224, 224])\n",
    "    Y = torch.zeros([len(sample), 6])\n",
    "    for sample_idx, (input_tensor, target) in enumerate(sample):\n",
    "        X[sample_idx] = input_tensor[0]\n",
    "        Y[sample_idx] = target[0]\n",
    "        print(target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
